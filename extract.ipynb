{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emb_size': 128, 'num_blocks': 6, 'num_bilinear': 8, 'num_spherical': 7, 'num_radial': 6, 'cutoff': 5.0, 'envelope_exponent': 5, 'num_before_skip': 1, 'num_after_skip': 2, 'num_dense_output': 3, 'num_train': 110000, 'num_valid': 10000, 'data_seed': 42, 'dataset': 'data/qm9_eV.npz', 'logdir': '.', 'num_steps': 3000000, 'ema_decay': 0.999, 'learning_rate': 0.001, 'warmup_steps': 3000, 'decay_rate': 0.01, 'decay_steps': 4000000, 'batch_size': 32, 'evaluation_interval': 10000, 'save_interval': 10000, 'restart': 'None', 'comment': 'final', 'targets': ['U0']}\n"
     ]
    }
   ],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_bilinear = config['num_bilinear']\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset_path = config['dataset']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "#####################################################################\n",
    "targets = config['targets']  # Change this if you want to predict a different target, e.g. to ['U0']\n",
    "#####################################################################\n",
    "targets = ['lumo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_container = DataContainer(dataset_path, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into training, validation and test set based on data_seed)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "\n",
    "# Initialize datasets\n",
    "dataset = data_provider.get_dataset('test').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_iter = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DimeNet(emb_size=emb_size, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "                num_spherical=num_spherical, num_radial=num_radial,\n",
    "                cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "                num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "                num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "                activation=swish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights from model at best step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa4804809b0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Load the trained model from your own training run\n",
    "# directory = \"/path/to/log/dir\"  # Fill this in\n",
    "# best_ckpt_file = os.path.join(directory, 'best', 'ckpt')\n",
    "#####################################################################\n",
    "# Uncomment this if you want to use a pretrained model\n",
    "directory = f\"pretrained/{targets[0]}\"\n",
    "best_ckpt_file = os.path.join(directory, 'ckpt')\n",
    "#####################################################################\n",
    "\n",
    "model.load_weights(best_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize aggregates\n",
    "metrics = Metrics('val', targets)\n",
    "pred_list, input_list, target_list, P_list, feature_list, sum_feature_list = [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868add33c8cc416d95d9963ec03a31cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=339.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = int(np.ceil(data_provider.nsamples['test'] / batch_size))\n",
    "\n",
    "for step in tqdm(range(steps_per_epoch)):\n",
    "    preds, inputs, target, P, features, sum_features = trainer.extract_on_batch(dataset_iter, metrics)\n",
    "    pred_list.append(preds)\n",
    "    input_list.append(inputs)\n",
    "    target_list.append(target)\n",
    "    P_list.append(P)\n",
    "    feature_list.append(features)\n",
    "    sum_feature_list.append(sum_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e678e45c0c11482db750a8cd84d3048b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tensor2numpy(tensor):\n",
    "    return tf.make_ndarray(tf.make_tensor_proto(tensor))\n",
    "\n",
    "data_list = []\n",
    "for preds, inputs, target, P, features, sum_features in tqdm(zip(pred_list, input_list, target_list, P_list, feature_list, sum_feature_list)):\n",
    "    batch_seg = tensor2numpy(inputs['batch_seg'])\n",
    "    idnb_i, idnb_j = tensor2numpy(inputs['idnb_i']), tensor2numpy(inputs['idnb_j'])\n",
    "    preds, target, P = tensor2numpy(preds), tensor2numpy(target), tensor2numpy(P)\n",
    "    features, sum_features = tensor2numpy(features), tensor2numpy(sum_features)\n",
    "    sample_num = np.max(batch_seg)+1\n",
    "    for si in range(sample_num):\n",
    "        assign_vector = (batch_seg==si).astype('float')\n",
    "        diff_vector = np.abs(assign_vector[:-1]-assign_vector[1:])\n",
    "        assert(np.sum(diff_vector) <= 2)\n",
    "    node_num_list = [np.sum(batch_seg==si) for si in range(sample_num)]\n",
    "    split_index = [0,]+np.cumsum(node_num_list).tolist()\n",
    "    for si in range(sample_num):\n",
    "        edge_flag = np.logical_and(idnb_i < split_index[si+1], idnb_i >= split_index[si])\n",
    "        assert(np.all(edge_flag==np.logical_and(idnb_j < split_index[si+1], idnb_j >= split_index[si])))\n",
    "        data_list.append({\n",
    "            'feat': features[batch_seg==si],\n",
    "            'out_feat': sum_features[batch_seg==si],\n",
    "            'final_feat': P[batch_seg==si],\n",
    "            'pred': preds[si],\n",
    "            'y': target[si],\n",
    "            'edge_index': np.stack([\n",
    "                idnb_j[edge_flag]-split_index[si],\n",
    "                idnb_i[edge_flag]-split_index[si],\n",
    "            ], axis=0)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle as pkl\n",
    "\n",
    "if not os.path.exists(f'extracted_features'):\n",
    "    os.mkdir(f'extracted_features')\n",
    "filename = f\"extracted_features/{targets[0]}.pkl\"\n",
    "with open(filename, 'wb') as f:\n",
    "    pkl.dump(data_list, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
